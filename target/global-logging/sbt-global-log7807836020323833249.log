[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-1)))[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Processing event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///Users/marielynn/Documents/GitHub/data-eng/src/main/scala/com/peaceland/kafka/processing/KafkaConsumerStorage.scala","languageId":"scala","version":1,"text":"package com.peaceland.kafka.processing\nimport org.apache.spark.sql.{DataFrame, Encoders, SparkSession}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.streaming._\nimport com.peaceland.utils._\n\nobject KafkaConsumerStorage\n{\n  def main(args: Array[String]): Unit =\n  {\n    //Creation du spark session\n    val spark = SparkSession\n      .builder()\n      .appName(\"Peacland_storage\")\n      .master(\"local[*]\")\n      .config(\"some.config.option\", \"some-value\")\n      .getOrCreate()\n\n    //Utilisation de la case class Report pour dÃ©finir le schÃ©ma de notre spark data frame\n    val mySchema = Encoders. product[Report].schema\n\n    //RÃ©cuparation des donnÃ©es de la stream\n    val topiC = \"peaceLand\"\n    val bootstrapServer = \"localhost:9092\"\n\n    val kafkaToDf : DataFrame = spark\n      .readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", bootstrapServer)\n      .option(\"subscribe\", topiC)\n      .option(\"startingOffsetes\", \"earliest\")\n      .load()\n\n    //TRansformation des valeurs rÃ©cupÃ©rÃ©es\n    val df = kafkaToDf\n      .select(from_json(col(\"value\").cast(\"string\"),mySchema).as(\"Data_report\"))\n      .select(\"Data_report.*\")\n      .withColumnRenamed(\"droneId\",\"id\")\n      .withColumn(\"id\",col(\"id\").cast(StringType))\n\n\n    //AZURE\n    val cosmosEndpoint = \"https://data-ing.documents.azure.com:443/\"\n    val cosmosMasterKey = \"1s9ljz9iHzSINDtnGYiCrJOHy9oEzJiSAF9LqPbD7yyEGN1FolSyFNeM4SMgXtkOVZXHWHKQdWLKACDbc9j5LQ==\" //DÃ©finir une secrets github?\n    val cosmosDatabaseName = \"sampleDB\"\n    val cosmosContainerName = \"sampleContainer\"\n\n    val cfg = Map(\"spark.cosmos.accountEndpoint\" -> cosmosEndpoint,\n      \"spark.cosmos.accountKey\" -> cosmosMasterKey,\n      \"spark.cosmos.database\" -> cosmosDatabaseName,\n      \"spark.cosmos.container\" -> cosmosContainerName\n    )\n\n    // Configure Catalog Api to be used\n    spark.conf.set(s\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n    spark.conf.set(s\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint)\n    spark.conf.set(s\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey)\n\n    spark.createDataFrame(Seq((\"cat-alive1\", \"Schrodinger cat\", 2, true), (\"cat-dead1\", \"Schrodinger cat\", 2, false)))\n      .toDF(\"id\", \"name\", \"age\", \"isAlive\")\n      .write\n      .format(\"cosmos.oltp\")\n      .options(cfg)\n      .mode(\"APPEND\")\n      .save()\n\n    //Ecriture des donnÃ©es dans la stream\n    df\n      .writeStream\n      .format(\"cosmos.oltp\")\n      .options(cfg)\n      .option(\"checkpointLocation\", \"./checkpoints/reports\")\n      .outputMode(OutputMode.Append())\n      .start()\n      .awaitTermination()\n\n    //l'Ã©ceiture dans la database se fait de maniÃ¨re continue\n    //solution? rajouter une contrainte temporelle avec java timer?\n    spark.close()\n  }\n}\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0manalysis location (/Users/marielynn/Documents/GitHub/data-eng/target/scala-2.12/zinc/inc_compile_2.12.zip,true)[0m
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 1 s, completed 27 mai 2023, 11:34:59[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Done event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didClose: JsonRpcNotificationMessage(2.0, textDocument/didClose, {"textDocument":{"uri":"file:///Users/marielynn/Documents/GitHub/data-eng/src/main/scala/com/peaceland/kafka/processing/KafkaConsumerStorage.scala"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///Users/marielynn/Documents/GitHub/data-eng/src/main/scala/com/peaceland/kafka/processing/KafkaProducerApp.scala","languageId":"scala","version":1,"text":"\npackage com.peaceland.kafka.processing\n\nimport com.peaceland.utils._\nimport org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord, RecordMetadata}\nimport org.apache.kafka.common.serialization.StringSerializer\nimport play.api.libs.json.Json\n\nimport java.util.{Properties, Timer, TimerTask}\n\nobject KafkaProducerApp //extends App\n{\n  //CrÃ©ation et configuration du producer\n  val props: Properties = new Properties()\n  props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\")\n  props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer])\n  props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer])\n  props.put(\"acks\",\"all\")\n\n  val producer: KafkaProducer[String, String] = new KafkaProducer[String, String](props)// producer instance\n\n  val nbDrones =10 //Number of drones in activity\n  val topiC =\"peaceLand\"\n  def droneData(): Unit = {\n    println(\"Sending data to topic peaceLand\")\n    val drones = Drone.generateDrone(nbDrones)\n    drones.foreach(drone => droneDataToKafka(drone))\n  }\n\n  def droneDataToKafka(drone: Drone): Unit = {\n    implicit val reportImplicitWrites = Json.writes[Report]\n    println(reportImplicitWrites)\n    val droneReport = Json.toJson(Report.generateReport(drone)) //Converting repport in JSON format\n    println(droneReport)\n    val record: ProducerRecord[String, String] = new ProducerRecord[String, String](topiC, drone.droneId.toString, droneReport.toString)\n    producer.send(record, (recordMetadata: RecordMetadata, exception: Exception) => {\n      if (exception != null) {\n        exception.printStackTrace()\n      }\n      else {\n        println(s\"Metadata about the sent record : $recordMetadata\")\n      }\n    })\n    println(\"sending data via kafka\" + droneReport.toString())\n\n    producer.flush()\n  }\n\n\n  def main(args: Array[String]): Unit = {\n    println(\"Starting producer ............\")\n    droneData()\n    val timer = new Timer()\n    val timerTask = new TimerTask { //Difining the timer task\n      override def run(): Unit = { //override is used to redifine the existing function run()\n        droneData()\n      }\n    }\n    timer.schedule(timerTask, 50, 30000)\n    /*\n    * The first repport willl be sent after 50 milisecond\n    * There is 5000 ms (5s) between the sending of each repport\n    * */\n  }\n  //Envoi d'un message\n  /*val key = \"akey\"\n  val value = \"my second message\"\n  val topic=\"text_topic\"\n\n  val record: ProducerRecord[String, String] = new ProducerRecord[String, String](topic, key, value)\n  producer.send(record, (recordMetadata: RecordMetadata, exception: Exception) => {\n    if (exception != null) {\n      exception.printStackTrace()\n    }\n    else {\n      println(s\"Metadata about the sent record : $recordMetadata\")\n    }\n  })*/\n\n}\n"}})[0m
